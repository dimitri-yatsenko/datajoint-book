{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Computation as Workflow\n\nDataJoint's central innovation is to recast relational databases as executable workflow specifications comprising a mixture of manual and automated steps. This chapter connects the theoretical foundations of the [Relational Workflow Model](../20-concepts/04-workflows.md) to practical computation patterns.\n\n## The Relational Workflow Model in Action\n\nRecall that the **Relational Workflow Model** is built on four fundamental concepts:\n\n1. **Workflow Entity** — Each table represents an entity type created at a specific workflow step\n2. **Workflow Dependencies** — Foreign keys prescribe the order of operations\n3. **Workflow Steps** — Distinct phases where entity types are created (manual or automated)\n4. **Directed Acyclic Graph (DAG)** — The schema forms a graph structure ensuring valid execution sequences\n\nThe Relational Workflow Model defines a new class of databases: **Computational Databases**, where computational transformations are first-class citizens of the data model. In a computational database, the schema is not merely a passive data structure—it is an executable specification of the workflow itself.\n\n## From Declarative Schema to Executable Pipeline\n\nA DataJoint schema uses **table tiers** to distinguish different workflow roles:\n\n| Tier | Color | Role in Workflow |\n|------|-------|------------------|\n| **Lookup** | Gray | Static reference data and configuration parameters |\n| **Manual** | Green | Human-entered data or data from external systems |\n| **Imported** | Blue | Data acquired automatically from instruments or files |\n| **Computed** | Red | Derived data produced by computational transformations |\n\nBecause dependencies are explicit through foreign keys, DataJoint's `populate()` method can explore the DAG top-down: for every upstream key that has not been processed, it executes the table's `make()` method inside an atomic transaction. If anything fails, the transaction is rolled back, preserving **computational validity**—the guarantee that all derived data remains consistent with its upstream dependencies.\n\nThis is the essence of **workflow automation**: each table advertises what it depends on, and `populate()` runs only the computations that are still missing. The [Blob-detection Pipeline](../80-examples/075-blob-detection.ipynb) from the examples chapter demonstrates how this plays out in practice.\n\n## Case Study: Blob Detection\n\nThe notebook `075-blob-detection.ipynb` assembles a compact image-analysis workflow:\n\n1. **Store source imagery** – `Image` is a manual table with a `longblob` field. NumPy arrays fetched from `skimage` are serialized automatically, illustrating that binary payloads need a serializer when stored in a relational database.\n2. **Scan parameter space** – `BlobParamSet` is a lookup table of min/max sigma and threshold values for `skimage.feature.blob_doh`. Each combination represents an alternative experiment configuration.\n3. **Compute detections** – `Detection` depends on both upstream tables. Its part table `Detection.Blob` holds every circle (x, y, radius) produced by the detector so that master and detail rows stay in sync.\n\n```python\n@schema\nclass Detection(dj.Computed):\n    definition = \"\"\"\n    -> Image\n    -> BlobParamSet\n    ---\n    nblobs : int\n    \"\"\"\n\n    class Blob(dj.Part):\n        definition = \"\"\"\n        -> master\n        blob_id : int\n        ---\n        x : float\n        y : float\n        r : float\n        \"\"\"\n\n    def make(self, key):\n        img = (Image & key).fetch1(\"image\")\n        params = (BlobParamSet & key).fetch1()\n        blobs = blob_doh(img,\n                         min_sigma=params['min_sigma'],\n                         max_sigma=params['max_sigma'],\n                         threshold=params['threshold'])\n        self.insert1(dict(key, nblobs=len(blobs)))\n        self.Blob.insert(dict(key, blob_id=i, x=x, y=y, r=r)\n                         for i, (x, y, r) in enumerate(blobs))\n```\n\nRunning `Detection.populate(display_progress=True)` fans out over every `(image, paramset)` pair, creating six jobs in the demo notebook. Because each job lives in an atomic transaction, half-written results never leak—this is the **isolation** guarantee that maintains workflow integrity.\n\n## Curate the Preferred Result\n\nAfter inspecting the plots, a small manual table `SelectDetection` records the \"best\" parameter set for each image. That drives a final visualization that renders only the chosen detections. This illustrates a common pattern: let automation explore the combinatorics, then capture human judgment in a concise manual table.\n\n## Why Computational Databases Matter\n\nThe Relational Workflow Model provides several key benefits:\n\n- **Reproducibility** — Rerunning `populate()` regenerates every derived table from raw inputs, providing a clear path from primary data to results\n- **Dependency-aware scheduling** — You do not need to script job order; DataJoint infers it from foreign keys (the DAG structure)\n- **Computational validity** — Foreign key constraints combined with immutable workflow artifacts ensure downstream results remain consistent with upstream inputs\n- **Provenance tracking** — The schema itself documents what was computed from what\n\n## Practical Tips\n\n- Develop `make()` logic with restrictions (e.g., `Detection.populate(key)`) before unlocking the entire pipeline\n- Use `display_progress=True` when you need visibility; use `reserve_jobs=True` when distributing work across multiple machines\n- If your computed table writes both summary and detail rows, keep them in a part table so the transaction boundary protects them together (see [Master-Part Relationships](../30-database-design/053-master-part.ipynb))\n\nThe blob-detection notebook is a self-contained template: swap in your own raw data source, adjust the parameter search, and you have the skeleton for an end-to-end computational database ready for scientific workflows.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}