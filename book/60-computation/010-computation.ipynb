{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation as Workflow\n",
    "\n",
    "**Draft — subject to revision**\n",
    "\n",
    "DataJoint reframes databases as *workflows*: each table advertises what it depends on, and DataJoint’s `populate()` method runs only the computations that are still missing. The [Blob-detection Pipeline](../80-examples/075-blob-detection.ipynb) from the examples chapter demonstrates how this plays out in practice and meets the demands of scientific reproducibility (reproducible processing and a clear path from primary data to interface).\n",
    "\n",
    "## From Declarative Schema to Executable Pipeline\n",
    "\n",
    "A DataJoint schema mixes several table roles:\n",
    "\n",
    "- **Manual / lookup tables** capture authoritative inputs and configuration options.\n",
    "- **Computed tables** declare derived data and embed the logic that produces it.\n",
    "- **Part tables** attach one-to-many detail that should always be inserted atomically with their parent.\n",
    "\n",
    "Because dependencies are explicit, `populate()` can explore the graph top-down: for every upstream key that has not been processed, it executes the table’s `make()` method; if anything fails, the transaction is rolled back.\n",
    "\n",
    "## Case Study: Blob Detection\n",
    "\n",
    "The notebook `075-blob-detection.ipynb` assembles a compact image-analysis workflow:\n",
    "\n",
    "1. **Store source imagery** – `Image` is a manual table with a `longblob` field. NumPy arrays fetched from `skimage` are serialized automatically, illustrating the lecture’s warning that binary payloads need a serializer when you save them in a relational database.\n",
    "2. **Scan parameter space** – `BlobParamSet` is a lookup table of min/max sigma and threshold values for `skimage.feature.blob_doh`. Each combination represents an alternative experiment configuration—exactly the “experiment parameters” mindset stressed in class.\n",
    "3. **Compute detections** – `Detection` depends on both upstream tables. Its part table `Detection.Blob` holds every circle (x, y, radius) produced by the detector so that master and detail rows stay in sync.\n",
    "\n",
    "```python\n",
    "@schema\n",
    "class Detection(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Image\n",
    "    -> BlobParamSet\n",
    "    ---\n",
    "    nblobs : int\n",
    "    \"\"\"\n",
    "\n",
    "    class Blob(dj.Part):\n",
    "        definition = \"\"\"\n",
    "        -> master\n",
    "        blob_id : int\n",
    "        ---\n",
    "        x : float\n",
    "        y : float\n",
    "        r : float\n",
    "        \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        img = (Image & key).fetch1(\"image\")\n",
    "        params = (BlobParamSet & key).fetch1()\n",
    "        blobs = blob_doh(img,\n",
    "                         min_sigma=params['min_sigma'],\n",
    "                         max_sigma=params['max_sigma'],\n",
    "                         threshold=params['threshold'])\n",
    "        self.insert1(dict(key, nblobs=len(blobs)))\n",
    "        self.Blob.insert(dict(key, blob_id=i, x=x, y=y, r=r)\n",
    "                         for i, (x, y, r) in enumerate(blobs))\n",
    "```\n",
    "\n",
    "Running `Detection.populate(display_progress=True)` fans out over every `(image, paramset)` pair, creating six jobs in the demo notebook. Because each job lives in a transaction, half-written results never leak—one of the isolation guarantees highlighted in the lecture’s ACID recap.\n",
    "\n",
    "## Curate the Preferred Result\n",
    "\n",
    "After inspecting the plots, a small manual table `SelectDetection` records the “best” parameter set for each image. That drives a final visualization that renders only the chosen detections. This illustrates a common pattern for the final project: let automation explore the combinatorics, then capture human judgment in a concise manual table. In the presentation, this curated view is what you would surface through Dash, Streamlit, or another GUI toolkit.\n",
    "\n",
    "## Why It Matters for the Final Project\n",
    "\n",
    "- **Reproducibility** – rerunning `populate()` regenerates every derived table from raw inputs, satisfying the requirement for trustworthy analyses.\n",
    "- **Dependency-aware scheduling** – you do not need to script job order; DataJoint infers it from foreign keys, exactly as promised in lecture.\n",
    "- **Extensibility** – adding a new image or parameter set triggers only the necessary new jobs, so the pipeline scales to the “at least six tables” complexity target.\n",
    "\n",
    "## Practical Tips\n",
    "\n",
    "- Develop `make()` logic with restrictions (e.g., `Detection.populate(key)`) before unlocking the entire pipeline.\n",
    "- Use `display_progress=True` when you need visibility; use `reserve_jobs=True` when distributing work across multiple machines.\n",
    "- If your computed table writes both summary and detail rows, keep them in a part table so the transaction boundary protects them together.\n",
    "\n",
    "The blob-detection notebook is a self-contained template: swap in your own raw data source, adjust the parameter search, and you have the skeleton for an end-to-end computational database ready to feed a dashboard demo on presentation day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
