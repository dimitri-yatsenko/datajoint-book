{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Delete\n\nThe `delete` operation removes entities from the database along with **all their downstream dependents**.\nThis cascading behavior is fundamental to maintaining **computational validity**—the guarantee that derived data remains consistent with its inputs.\n\n## Cascading Delete and Computational Validity\n\nIn the [Relational Workflow Model](../20-concepts/05-workflows.md), every entity in a Computed or Imported table was derived from specific upstream data.\nIf that upstream data is deleted or found to be incorrect, the derived results become meaningless—they are artifacts of inputs that no longer exist or were never valid.\n\nDataJoint enforces this principle through **cascading deletes**:\n\n```\nSubject ← Session ← Recording ← SpikeSort ← UnitAnalysis\n   │         │          │           │            │\n   └─────────┴──────────┴───────────┴────────────┘\n              Deleting a Session removes all of these\n```\n\nWhen you delete an entity:\n1. All entities that reference it (via foreign keys) are identified\n2. Those entities are recursively deleted\n3. The cascade continues through the entire dependency graph\n4. The final state is always referentially consistent\n\nThis is not merely cleanup—it is **enforcing the semantics of the workflow**.\nComputed results only have meaning in relation to their inputs.\n\n## The `delete` Method\n\n```python\n<Table>.delete(safemode=True, quick=False)\n```\n\n**Parameters:**\n- **`safemode`** (default: `True`): Prompts for confirmation before deleting\n- **`quick`** (default: `False`): If `True`, skips dependency analysis (use with caution)\n\n**Examples:**\n\n```python\n# Delete with confirmation prompt\n(Session & {'subject_id': 'M001', 'session_date': '2024-01-15'}).delete()\n\n# Delete without confirmation (scripted use)\n(Session & {'subject_id': 'M001', 'session_date': '2024-01-15'}).delete(safemode=False)\n\n# Delete all entries in a table (with confirmation)\nSession.delete()\n```\n\n## Use Cases for Delete\n\n### 1. Correcting Upstream Errors\n\nThe most common use of delete is correcting errors in upstream data.\nRather than updating values (which would leave downstream computations inconsistent), you:\n\n1. **Delete** the incorrect upstream data (cascade removes all derived results)\n2. **Insert** the corrected data\n3. **Repopulate** to regenerate downstream computations\n\n```python\n# Discovered an error in session metadata\n(Session & bad_session_key).delete(safemode=False)\n\n# Insert corrected data\nSession.insert1(corrected_session_data)\n\n# Regenerate all downstream analysis\nRecording.populate()\nSpikeSort.populate()\nUnitAnalysis.populate()\n```\n\n### 2. Reprocessing with Updated Code\n\nWhen you update your analysis code, you may want to regenerate computed results:\n\n```python\n# Delete computed results to force recomputation\n(SpikeSort & restriction).delete(safemode=False)\n\n# Repopulate with updated make() method\nSpikeSort.populate()\n```\n\n### 3. Removing Obsolete Data\n\nWhen data is no longer needed:\n\n```python\n# Remove old pilot data\n(Subject & 'subject_id LIKE \"pilot%\"').delete()\n```\n\n### 4. Selective Deletion with Restrictions\n\nUse DataJoint's restriction syntax to target specific subsets:\n\n```python\n# Delete only failed recordings\n(Recording & 'quality < 0.5').delete()\n\n# Delete sessions from a specific date range\n(Session & 'session_date < \"2023-01-01\"').delete()\n\n# Delete based on joined conditions\n(SpikeSort & (Recording & 'brain_region = \"V1\"')).delete()\n```\n\n## The Delete-Reinsert-Repopulate Pattern\n\nThis pattern is the standard way to handle corrections in DataJoint:\n\n```python\ndef correct_session(session_key, corrected_data):\n    \"\"\"Correct session data and regenerate all downstream analysis.\"\"\"\n    \n    # 1. Delete the session (cascades to all downstream)\n    (Session & session_key).delete(safemode=False)\n    \n    # 2. Insert corrected data\n    Session.insert1(corrected_data)\n    \n    # 3. Repopulate downstream tables\n    # DataJoint's populate() automatically determines what needs to run\n    Recording.populate()\n    ProcessedRecording.populate()\n    Analysis.populate()\n```\n\nThis pattern ensures:\n- No orphaned or inconsistent computed results\n- Full audit trail (original data is gone, not hidden)\n- All downstream results reflect the corrected inputs\n\n## Preview Before Deleting\n\nAlways verify what will be deleted before executing:\n\n```python\n# First, check what matches your restriction\nSession & {'subject_id': 'M001'}\n\n# Check downstream dependencies that will also be deleted\n(Session & {'subject_id': 'M001'}).descendants()\n\n# Then delete when confident\n(Session & {'subject_id': 'M001'}).delete()\n```\n\n## Safety Mechanisms\n\nDataJoint provides several safeguards:\n\n1. **`safemode=True`** (default): Requires interactive confirmation showing what will be deleted\n2. **Dependency preview**: Shows the count of entries in dependent tables that will be affected\n3. **Transaction wrapping**: The entire cascading delete is atomic—it either fully succeeds or fully rolls back\n\n## Best Practices\n\n1. **Trust the cascade**: Don't manually delete downstream tables first—let DataJoint handle dependencies\n2. **Use restrictions**: Target specific subsets rather than deleting entire tables\n3. **Preview first**: Check what matches before deleting, especially with complex restrictions\n4. **Keep `safemode=True`** for interactive work: Only use `safemode=False` in tested scripts\n5. **Think in terms of workflow**: Deleting is not \"cleaning up\"—it's rolling back the workflow to an earlier state\n6. **Follow with repopulate**: After correcting data, run `populate()` to bring the pipeline back to a complete state"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}