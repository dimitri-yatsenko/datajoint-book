{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Populate\n\nThe `populate` operation is the engine of workflow automation in DataJoint.\nWhile `insert`, `delete`, and `update` are manual operations, `populate` automates data entry for **Imported** and **Computed** tables based on dependencies defined in the schema.\n\nThis chapter connects the theoretical foundations of the [Relational Workflow Model](../20-concepts/05-workflows.md) to the practical `populate` operation.\n\n## The Relational Workflow Model in Action\n\nRecall that the **Relational Workflow Model** is built on four fundamental concepts:\n\n1. **Workflow Entity** — Each table represents an entity type created at a specific workflow step\n2. **Workflow Dependencies** — Foreign keys prescribe the order of operations\n3. **Workflow Steps** — Distinct phases where entity types are created (manual or automated)\n4. **Directed Acyclic Graph (DAG)** — The schema forms a graph structure ensuring valid execution sequences\n\nThe Relational Workflow Model defines a new class of databases: **Computational Databases**, where computational transformations are first-class citizens of the data model. In a computational database, the schema is not merely a passive data structure—it is an executable specification of the workflow itself.\n\n## From Declarative Schema to Executable Pipeline\n\nA DataJoint schema uses **table tiers** to distinguish different workflow roles:\n\n| Tier | Color | Role in Workflow |\n|------|-------|------------------|\n| **Lookup** | Gray | Static reference data and configuration parameters |\n| **Manual** | Green | Human-entered data or data from external systems |\n| **Imported** | Blue | Data acquired automatically from instruments or files |\n| **Computed** | Red | Derived data produced by computational transformations |\n\nBecause dependencies are explicit through foreign keys, DataJoint's `populate()` method can explore the DAG top-down: for every upstream key that has not been processed, it executes the table's `make()` method inside an atomic transaction. If anything fails, the transaction is rolled back, preserving **computational validity**—the guarantee that all derived data remains consistent with its upstream dependencies.\n\nThis is the essence of **workflow automation**: each table advertises what it depends on, and `populate()` runs only the computations that are still missing.\n\n## The `populate` Method\n\nThe `populate()` method is the engine of workflow automation. When called on a computed or imported table, it:\n\n1. **Identifies missing work** — Queries the key source (the join of all upstream dependencies) and subtracts keys already present in the table\n2. **Iterates over pending keys** — For each missing key, calls the table's `make()` method\n3. **Wraps each `make()` in a transaction** — Ensures atomicity: either all inserts succeed or none do\n4. **Handles errors gracefully** — Failed jobs are logged but do not stop the remaining work\n\n```python\n# Process all pending work\nDetection.populate(display_progress=True)\n\n# Process a specific subset\nDetection.populate(Image & \"image_id < 10\")\n\n# Distribute across workers\nDetection.populate(reserve_jobs=True)\n```\n\nThe `reserve_jobs=True` option enables parallel execution across multiple processes or machines by using the database itself for job coordination.\n\n## Anatomy of a `make` Method\n\nThe `make()` method is where the actual computation happens.\nIts input is a single argument: the **key** dict identifying which entity to compute.\nThis key contains the primary key attributes from the key source—the join of all upstream dependencies.\n\nA well-structured `make()` method has three distinct parts:\n\n### 1. Fetch\n\nRetrieve the necessary data from upstream tables using the provided key:\n\n```python\ndef make(self, key):\n    # 1. FETCH: Get data from upstream tables\n    image = (Image & key).fetch1(\"image_data\")\n    params = (BlobParamSet & key).fetch1()\n```\n\nThe key restricts each upstream table to exactly the relevant row(s).\nUse `fetch1()` when expecting a single row, `fetch()` for multiple rows.\n\n### 2. Compute\n\nPerform the actual computation or data transformation:\n\n```python\n    # 2. COMPUTE: Perform the transformation\n    blobs = detect_blobs(\n        image,\n        min_sigma=params[\"min_sigma\"],\n        max_sigma=params[\"max_sigma\"],\n        threshold=params[\"threshold\"],\n    )\n```\n\nThis is the scientific or business logic—image processing, statistical analysis, simulation, or any transformation that produces derived data.\n\n### 3. Insert\n\nStore the results in the table (and any part tables):\n\n```python\n    # 3. INSERT: Store results\n    self.insert1({**key, \"blob_count\": len(blobs)})\n    self.Blob.insert([{**key, \"blob_id\": i, **b} for i, b in enumerate(blobs)])\n```\n\nThe key must be included in the inserted row to maintain referential integrity.\nFor master-part structures, insert both the master row and all part rows within the same `make()` call.\n\n### Complete Example\n\n```python\n@schema\nclass Detection(dj.Computed):\n    definition = \"\"\"\n    -> Image\n    -> BlobParamSet\n    ---\n    blob_count : int\n    \"\"\"\n\n    class Blob(dj.Part):\n        definition = \"\"\"\n        -> master\n        blob_id : int\n        ---\n        x : float\n        y : float\n        sigma : float\n        \"\"\"\n\n    def make(self, key):\n        # 1. FETCH\n        image = (Image & key).fetch1(\"image_data\")\n        params = (BlobParamSet & key).fetch1()\n\n        # 2. COMPUTE\n        blobs = detect_blobs(\n            image,\n            min_sigma=params[\"min_sigma\"],\n            max_sigma=params[\"max_sigma\"],\n            threshold=params[\"threshold\"],\n        )\n\n        # 3. INSERT\n        self.insert1({**key, \"blob_count\": len(blobs)})\n        self.Blob.insert([{**key, \"blob_id\": i, **b} for i, b in enumerate(blobs)])\n```\n\nThis three-part structure—**fetch, compute, insert**—keeps `make()` methods readable and maintainable.\nEach part has a clear responsibility, making it easy to debug and extend.\n\n## Transactional Integrity\n\nEach `make()` call executes inside an **ACID transaction**. This provides critical guarantees for computational workflows:\n\n- **Atomicity** — The entire computation either commits or rolls back as a unit\n- **Isolation** — Partial results are never visible to other processes\n- **Consistency** — The database moves from one valid state to another\n\nWhen a computed table has [part tables](../30-design/053-master-part.ipynb), the transaction boundary encompasses both the master and all its parts. The master's `make()` method is responsible for inserting everything within a single transactional scope. See the [Master-Part](../30-design/053-master-part.ipynb) chapter for detailed coverage of ACID semantics and the master's responsibility pattern.\n\n## Case Study: Blob Detection\n\nThe [Blob Detection](../80-examples/075-blob-detection.ipynb) example demonstrates these concepts in a compact image-analysis workflow:\n\n1. **Source data** — `Image` (manual) stores NumPy arrays as `longblob` fields\n2. **Parameter space** — `BlobParamSet` (lookup) defines detection configurations\n3. **Computation** — `Detection` (computed) depends on both upstream tables\n\nThe `Detection` table uses a master-part structure: the master row stores an aggregate (blob count), while `Detection.Blob` parts store per-feature coordinates. When `populate()` runs:\n\n- Each `(image_id, blob_paramset)` combination triggers one `make()` call\n- The `make()` method fetches inputs, runs detection, and inserts both master and parts\n- The transaction ensures all blob coordinates appear atomically with their count\n\n```python\nDetection.populate(display_progress=True)\n# Detection: 100%|██████████| 6/6 [00:01<00:00, 4.04it/s]\n```\n\nThis pattern—automation exploring combinatorics, then human curation—is common in scientific workflows. After reviewing results, the `SelectDetection` manual table records the preferred parameter set for each image. Because `SelectDetection` depends on `Detection`, it implicitly has access to all `Detection.Blob` parts for the selected detection.\n\n:::{seealso}\n- [Blob Detection](../80-examples/075-blob-detection.ipynb) — Complete working example\n- [Master-Part](../30-design/053-master-part.ipynb) — Transaction semantics and dependency implications\n:::\n\n## Why Computational Databases Matter\n\nThe Relational Workflow Model provides several key benefits:\n\n| Benefit | Description |\n|---------|-------------|\n| **Reproducibility** | Rerunning `populate()` regenerates derived tables from raw inputs |\n| **Dependency-aware scheduling** | DataJoint infers job order from foreign keys (the DAG structure) |\n| **Computational validity** | Transactions ensure downstream results stay consistent with upstream inputs |\n| **Provenance tracking** | The schema documents what was computed from what |\n\n## Practical Tips\n\n- **Develop incrementally** — Test `make()` logic with restrictions (e.g., `Table.populate(restriction)`) before processing all data\n- **Monitor progress** — Use `display_progress=True` for visibility during development\n- **Distribute work** — Use `reserve_jobs=True` when running multiple workers\n- **Use master-part for multi-row results** — When a computation produces both summary and detail rows, structure them as master and parts to keep them in the same transaction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}